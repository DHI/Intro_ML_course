{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Before starting*:\n",
        "\n",
        "We are running deep neural networks in this notebook to speed up training, it is recommended to change the runtime type from` CPU` to `T4 GPU` in Google Colab.\n",
        "\n",
        "You can do that by clicking the arrow which is below the settings sign on the top right of your screen, a menu should appear, click the second line - `change runtime type`. Below Hardware accelerator, select `T4 GPU`."
      ],
      "metadata": {
        "id": "zLUXsbYoaw4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6 - Convolutional neural networks\n"
      ],
      "metadata": {
        "id": "BQ1PHaOBCueJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/DHI/Intro_ML_course/blob/main/module_6/6_convolutional_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "yZ55XwcmMDOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to Module 6 of our course! In this module, we will delve into the fascinating world of Convolutional Neural Networks (CNNs), a powerful extension of the neural networks we explored in Module 4. Our focus will be on using CNNs for image classification, a task that becomes particularly challenging and interesting when dealing with complex visual data.\n",
        "\n",
        "In module 4, we strengthened our understanding of Multi-Layer Perceptrons (MLPs) and applied them to image classification using the CIFAR-10 dataset. Now, we are ready to take our skills to the next level by introducing CNNs. Unlike MLPs, CNNs are specifically designed for tasks involving images.\n",
        "\n",
        "For this module, we have an exciting challenge lined up. Instead of CIFAR-10 dataset, we will be working with a more complex set of images featuring dolphins, orcas, and whales. This task introduces a new level of complexity.\n",
        "\n",
        "### Goals of this notebook\n",
        "\n",
        "1. **Understand convolutions:** Gain a solid grasp of the key concept of convolutions, you will see how two $3 \\times 3$ filter can be used to automatically detect the edges in an image.\n",
        "\n",
        "2. **Learn how to define a convolutional neural network:**  Learn how to setup and train a convolutional neural network. You will see that it is exactly the same principle as for a multi-layer perceptron, only the architecture (or namely, the layers) of the model differ.\n",
        "\n",
        "3. **Explore Transfer Learning:** Delve into the realm of transfer learning, leveraging pre-trained CNN models to boost performance."
      ],
      "metadata": {
        "id": "rpBt1E52NFOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Machine learning / deep learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Flatten\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Image libraries\n",
        "from scipy.signal import convolve2d\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "RVtDqAHLL_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load and pre-process the images"
      ],
      "metadata": {
        "id": "sJQAlwiU72-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will lay the foundation for our marine life image classification task by loading and pre-processing the images of dolphins, orcas, and whales. The next lines of code will load the .zip file where all the image are stored.\n",
        "\n",
        "*Note*: This dataset is a subset of the [ImageNet dataset](https://ieeexplore.ieee.org/document/5206848), all the images were rescaled to $64 \\times 64$ to make allow the whole dataset to fit in memory."
      ],
      "metadata": {
        "id": "ETNGfyQeOHw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/DHI/Intro_ML_course/main/images/orca_dolphin_whale.zip\n",
        "!unzip orca_dolphin_whale.zip >/dev/null"
      ],
      "metadata": {
        "id": "-aXnGcHR4-uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will\n",
        "1. load the images\n",
        "2. scale them between zero and one\n",
        "3. and attribute a label (`{'dolphin': 0, 'orca': 1, 'whale': 2}`) to the data."
      ],
      "metadata": {
        "id": "v8Zzt73p8FJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(folder_path, label):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img = Image.open(os.path.join(folder_path, filename))\n",
        "        img = np.array(img) / 255.0  # 0-255 to 0-1\n",
        "        images.append((img, label))\n",
        "    return images\n",
        "\n",
        "all_images = []\n",
        "class_names = [\"dolphin\", \"orca\", \"whale\"]\n",
        "for label, animal in enumerate(class_names):\n",
        "    images = load_images(animal, label=label)\n",
        "    all_images.extend(images)"
      ],
      "metadata": {
        "id": "YN_TVxeANx1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, let's split the data into training and test datasets."
      ],
      "metadata": {
        "id": "w8YMF3W66oIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    np.array([img for img, _ in all_images]).astype('float32'),\n",
        "    np.array([label for _, label in all_images]),\n",
        "    test_size=0.33, random_state=42)\n",
        "del all_images"
      ],
      "metadata": {
        "id": "mO8V4HoH1FSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the images:"
      ],
      "metadata": {
        "id": "2n9ph5PZOv0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "for i in range(7*10):\n",
        "    plt.subplot(7, 10, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train[i])\n",
        "    plt.xlabel(f'{i}-{class_names[y_train[i]]}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A62f5rxfN2Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Introduction to convolutions: edge detection through Sobel filters"
      ],
      "metadata": {
        "id": "eSyBAzCHstqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we delve into the basics of convolutions and showcase their power in feature extraction using simple filters, specifically the Sobel operators.\n",
        "\n",
        "### Convolutions\n",
        "\n",
        "Convolutional operations involve sliding a filter (also known as a kernel or mask) over an image, computing the element-wise product of the filter and the image pixels within its receptive field, and then summing these products. This process enables the extraction of various features like edges, textures, and patterns.\n",
        "\n",
        "### Sobel Filters for Edge Detection\n",
        "\n",
        "Sobel operators are widely used for edge detection, a crucial step in image processing and computer vision tasks. These filters consist of two 3x3 matrices, one for detecting horizontal changes and the other for vertical changes. By applying these filters through convolution, we can emphasize edges in different orientations.\n",
        "\n",
        "### Loading and Preprocessing the Image\n",
        "\n",
        "Before we begin, we need to load an image from our dataset and we convert it to grayscale for convenience."
      ],
      "metadata": {
        "id": "dewWi2mr0dzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can choose the image of your preference based on the grid of images above\n",
        "n_image = 3"
      ],
      "metadata": {
        "id": "jfb6pX-4ukGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = X_train[n_image]\n",
        "image_grayscale = np.mean(X_train[n_image], axis=-1)"
      ],
      "metadata": {
        "id": "_CimStyTs6z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_filter(image, kernel):\n",
        "    # Apply the filter using scipy's convolve2d\n",
        "    filtered = convolve2d(image, kernel, mode='same', boundary='symm')\n",
        "    return convolve2d(image, kernel, mode='same', boundary='symm')"
      ],
      "metadata": {
        "id": "ptFb38vls1Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before applying Sobel operators to an image, let's take a closer look at the two 3x3 filters used for horizontal (Sobel-X) and vertical (Sobel-Y) edge detection. These kernels play a crucial role in highlighting changes in intensity in their respective directions."
      ],
      "metadata": {
        "id": "vHT55gRP0KL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 3x3 vertical and horizontal filters\n",
        "vertical_kernel = np.array([[1, 0, -1],\n",
        "                            [2, 0, -2],\n",
        "                            [1, 0, -1]])\n",
        "horizontal_kernel = np.array([[1, 2, 1],\n",
        "                              [0, 0, 0],\n",
        "                              [-1, -2, -1]])"
      ],
      "metadata": {
        "id": "PA0rmQofuuq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize these filters\n",
        "fig, ax = plt.subplots(1, 2, figsize=(5, 3))\n",
        "ax[0].imshow(vertical_kernel, cmap='PuOr')\n",
        "ax[0].set_title('Vertical Filter')\n",
        "ax[0].axis('off')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax[0].text(j, i, str(vertical_kernel[i, j]), color='orange',\n",
        "                    ha='center', va='center', fontsize=12, fontweight='bold')\n",
        "ax[1].imshow(horizontal_kernel, cmap='PuOr')\n",
        "ax[1].set_title('Horizontal Filter')\n",
        "ax[1].axis('off')\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax[1].text(j, i, str(horizontal_kernel[i, j]), color='orange',\n",
        "                    ha='center', va='center', fontsize=12, fontweight='bold')"
      ],
      "metadata": {
        "id": "P3HPm1tbtsXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the filter to the image using scipy's `convolve2d` function:"
      ],
      "metadata": {
        "id": "zKkF659X-C-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vertical_filtered = convolve2d(image_grayscale, vertical_kernel,\n",
        "                               mode='same', boundary='symm')\n",
        "horizontal_filtered = convolve2d(image_grayscale, horizontal_kernel,\n",
        "                                 mode='same', boundary='symm')"
      ],
      "metadata": {
        "id": "W1rxK6Sp-Bvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the results!"
      ],
      "metadata": {
        "id": "Quz-seZ3-ahX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the original image\n",
        "fig, ax = plt.subplots(1, 3, figsize=(12, 5))\n",
        "ax[0].imshow(image_grayscale, cmap='gray')\n",
        "ax[0].axis('off')\n",
        "ax[0].set_title('Original')\n",
        "\n",
        "# Display the result of the vertical Sobel filter convolution\n",
        "ax[1].imshow(vertical_filtered, cmap='gray')\n",
        "ax[1].set_title('Vertical filter convolution')\n",
        "ax[1].axis('off')\n",
        "\n",
        "# Display the result of the horizontal Sobel filter convolution\n",
        "ax[2].imshow(horizontal_filtered, cmap='gray')\n",
        "ax[2].set_title('Horizontal filter convolution')\n",
        "ax[2].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yJRj7ywosYJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-dimensional rate of change of pixel intensity across an image can be derived by combining the effects of vertical and horizontal changes in intensity. The resulting image provides a visual map of edges and transitions in the image:"
      ],
      "metadata": {
        "id": "inNrif4krrtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_image = np.sqrt(vertical_filtered**2 + horizontal_filtered**2)"
      ],
      "metadata": {
        "id": "1lvWjQXh-sjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(edge_image, cmap='gray')\n",
        "plt.title('Edge detection')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OfbG0137x2DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The strength of deep neural networks derive from their ability to automatically find useful features from the raw data.\n",
        "\n",
        "In image analysis, features represent visually distinctive attributes. For instance, recognizing a dolphin involves detecting the triangular shape of its dorsal fin. It turns out that it is usually easier to detect this from the edges rather than relying on raw pixels.\n",
        "\n",
        "We have seen in this chapter how efficiently edge detection can be performed through edge detection. We hope this gives you some insights on why convolutions are useful in image processing."
      ],
      "metadata": {
        "id": "lPt1RQU4_jt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Visualize the `MaxPooling` operation"
      ],
      "metadata": {
        "id": "KBTjRogrDYUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Convolutional Neural Networks (CNNs), MaxPooling is a crucial operation employed for downsampling feature maps. MaxPooling is typically applied after convolutional layers and activation functions.\n",
        "\n",
        "**Key Functions of MaxPooling**:\n",
        "\n",
        "1. **Downsampling:** MaxPooling reduces the size of the feature maps, leading to a more compact representation of the input data.\n",
        "\n",
        "2. **Translation Invariance:** By retaining only the maximum values within small regions, MaxPooling introduces a degree of translation invariance, making the network more robust to slight shifts in input patterns.\n",
        "\n",
        "3. **Increase the receptive field:** 3 by 3 pixels covers a larger proportion of the image if this image has 32 $\\times$ 32 pixels than if it has 64 $\\times$ 64. In a convolutional neural network, each MaxPooling layer allows the model to process increasingly larger proportions of the input image. This hierarchical approach facilitates the extraction of hierarchical features, enhancing the network's ability to capture both local details and broader contextual information in the data.\n",
        "\n",
        "Let's visualize the effect of a MaxPooling operation on our edge detection feature!"
      ],
      "metadata": {
        "id": "Q5wGFWKWDocG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map = edge_image\n",
        "feature_map.shape"
      ],
      "metadata": {
        "id": "vCP5Ty8ZEvg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hard-code max pooling** to reduce the dimensions from $(64, 64)$ to $(32, 32)$. This involves identifying, within each 2 by 2 pixels box, the pixel with the highest value."
      ],
      "metadata": {
        "id": "qVHJkqfKE0Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_feature_map = np.zeros((feature_map.shape[0] // 2, feature_map.shape[1] // 2))\n",
        "for i in range(0, feature_map.shape[0] - 1, 2):\n",
        "    for j in range(0, feature_map.shape[1] - 1, 2):\n",
        "        pooled_feature_map[i // 2, j // 2] = np.max(feature_map[i:i+2, j:j+2])"
      ],
      "metadata": {
        "id": "vNGSOAuaEyNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_feature_map.shape"
      ],
      "metadata": {
        "id": "gH1JvFUHGMxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].imshow(feature_map, cmap='gray', interpolation='none')\n",
        "ax[0].set_title('Original Feature Map')\n",
        "ax[0].axis('off')\n",
        "ax[1].imshow(pooled_feature_map, cmap='gray', interpolation='none')\n",
        "ax[1].set_title('Max Pooled Feature Map')\n",
        "ax[1].axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jwgce5HgDEOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Convolutional neural network"
      ],
      "metadata": {
        "id": "s8fXAlyGO2q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Building upon our understanding of convolutions for edge detection and the significance of MaxPooling in downsampling, we now embark on the exploration of Convolutional Neural Networks (CNNs) - sophisticated architectures designed for image classification and feature extraction.\n",
        "\n",
        "### Anatomy of CNNs:\n",
        "\n",
        "CNNs are characterized by a sequence of convolutional and MaxPooling layers, strategically arranged. This architectural design facilitates the extraction of intricate visual features, progressively revealing more abstract representations.\n",
        "\n",
        "#### Convolutional Layers:\n",
        "   - **Feature Extraction:** Conv2D layers are the cornerstone, systematically applying filters to the input images. These filters, akin to our previously explored vertical and horizontal kernels extract visual features in an efficient way. The parameters / weights that the model learns through training are the values of the filters.\n",
        "   - **Abstraction Levels:** As we move through successive convolutional layers, the network captures high-level features with an increasing level of abstraction. This process allows the CNN to learn complex visual patterns and structures.\n",
        "\n",
        "#### MaxPooling Layers:\n",
        "   - **Receptive Field Expansion:** MaxPooling layers contribute to downsampling spatial dimensions, enabling the network to process larger portions of the input. This leads to an increased receptive field, facilitating the understanding of contextual relationships within the image.\n",
        "\n",
        "#### Flattening and Dense Layers:\n",
        "   - **Transition to MLP-like Structure:** Following the feature extraction process, the flattened output is fed into dense layers, resembling the structure of Multi-Layer Perceptrons (MLPs) from Module 4.\n",
        "   - **Class Relationship:** These dense layers analyze the relationships between the extracted visual features and the predefined classes, allowing the CNN to make predictions.\n"
      ],
      "metadata": {
        "id": "oqpWEIGIwcjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a convolutional neural network\n",
        "CNN_model = keras.Sequential([\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "VTByU9IpHTtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `model.summary()` output offers a comprehensive view of the Convolutional Neural Network (CNN) architecture, revealing the hierarchical treatment of the input data. Let's delve into the breakdown:"
      ],
      "metadata": {
        "id": "IQkRuJSyyx3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model.summary()"
      ],
      "metadata": {
        "id": "xchcNuZa1aV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Output Shape\" column delineates the dimensions of the output after each layer's operation. Note the systematic reduction in spatial resolution (62x62 ➔ 31x31 ➔ 14x14 ➔ 6x6) within a hierarchical structure, accompanied by an augmentation in the number of features. There is a transition from raw and fine data to high-level and coarse visual information, from maps of simple patterns to the detection complex shapes.\n",
        "\n",
        "The \"flatten\" layer plays a pivotal role by reshaping the 256 two-dimensional feature maps into a compressed 1D array containing 9216 elements.  These 9216 elements are high-level visual features, each corresponding to distinct regions in the image. The following dense layer comprehensively processes the entire image, combining insights from diverse parts to form a holistic representation. The ultimate dense layer establishes the crucial link between the aggregated global features and the classes."
      ],
      "metadata": {
        "id": "k5SsZ5tUyZfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest is the same as in module 4, we compile the model, indicating the `optimizer`, the `learning_rate` and the `loss` function."
      ],
      "metadata": {
        "id": "WRlz0LkY10kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "c2S43TSd1oyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still as in module 4, we fit the model using `keras` magical function `.fit`, and specifying the `batch_size`."
      ],
      "metadata": {
        "id": "TUwfw7t02FVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_history = CNN_model.fit(X_train, y_train,\n",
        "                            batch_size=16, epochs=20,\n",
        "                            validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "EAyd7jwb2Ep5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(CNN_history.history['accuracy'], label='train')\n",
        "plt.plot(CNN_history.history['val_accuracy'], label='validation')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.title('Learning curves')"
      ],
      "metadata": {
        "id": "jbWQBL0r3Dz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a trained model (even though it may not have converged and is probably overfitting at this stage).\n",
        "\n",
        "Let's start making predictions on the test dataset."
      ],
      "metadata": {
        "id": "RuStrpaD3b5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model performance"
      ],
      "metadata": {
        "id": "6qyEh7YQ5RbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_predictions = CNN_model.predict(X_test)"
      ],
      "metadata": {
        "id": "hAbaVxuwaxSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the following block multiple times to check the results on various images**"
      ],
      "metadata": {
        "id": "yFmBrwYZ3wTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick one instance in the test dataset\n",
        "random_index = np.random.randint(len(X_test))\n",
        "\n",
        "# display probabilities\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].bar(class_names, CNN_predictions[random_index], color='skyblue')\n",
        "ax[0].set_title(f'Predicted probabilities for image {random_index} of test dataset')\n",
        "ax[0].set_ylabel('Probabilities')\n",
        "ax[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
        "\n",
        "# show image\n",
        "ax[1].imshow(X_test[random_index])"
      ],
      "metadata": {
        "id": "zb1B65rKbG_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict classes from probabilities\n",
        "CNN_classes = np.argmax(CNN_predictions, axis=-1)"
      ],
      "metadata": {
        "id": "X42h_zGXdKbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Display the confusion matrix*"
      ],
      "metadata": {
        "id": "lSNOdS_z0sIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display confusion matrix"
      ],
      "metadata": {
        "id": "elhjNQPcdIfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My model had more confusion between orcas and whales than dolphins and whales, how about yours?"
      ],
      "metadata": {
        "id": "LYZbL8QF4ONf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Optional exercise**: Compare the performance of your CNN with a multi-layer perceptron (similar to module 4) - are CNNs that superior to MLPs?"
      ],
      "metadata": {
        "id": "WAxfEua0LTRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "PalYUsBaLSHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Transfer learning"
      ],
      "metadata": {
        "id": "-um1_nX1ByOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of training a model from scratch by initializing it with random weights, it might be better to efficient utilization of knowledge acquired from one task to enhance performance for another.\n",
        "\n",
        "In this paradigm, we leverage a pre-trained neural network, trained on the large ImageNet dataset, and tailor it to address our specific problem of classifying marine life."
      ],
      "metadata": {
        "id": "hV4VDoiZ8Y8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1. Load Pre-trained Model:\n",
        "\n",
        "We begin by importing a pre-trained model, in this case, `VGG16`, without its top layers (`include_top=False`). That is because `VGG16` has been trained on larger images (size: `224, 224, 3`). `VGG16` is a deep convolutional network trained on the large dataset ImageNet to recongnize 1,000 different classes. It is thus able to extract complex visual information and that will be useful for our task."
      ],
      "metadata": {
        "id": "trIaEwan5FXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.VGG16(weights='imagenet', include_top=False,\n",
        "                                      input_shape=(64, 64, 3))"
      ],
      "metadata": {
        "id": "mPqqZ_Q-6NKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it looks like:"
      ],
      "metadata": {
        "id": "2qO9Mzah8hsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "PxvpycOU8duP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The last layer of VGG 16 outputs 512 high-level feature maps of size (2, 2)\n",
        "VGG_16_features = base_model.layers[-1].output"
      ],
      "metadata": {
        "id": "-l1Bym5F9d1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2. Load Pre-trained Model:Add Custom Classification Layers:\n",
        "\n",
        "Next, we append custom classification layers to the pre-trained model. This involves creating a new softmax layer tailored to the number of classes in our specific problem (here: 3). But before this, we need to flatten the `(2, 2, 512)` shape of the last layer of our base model to a list of `(2048)` features."
      ],
      "metadata": {
        "id": "_PiQvynO6iIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten and dense layer\n",
        "flattened_features = Flatten()(VGG_16_features)\n",
        "probabilities = Dense(3, activation='softmax')(flattened_features)"
      ],
      "metadata": {
        "id": "d9hFqpZY9Vc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stitch the two parts of the model together:"
      ],
      "metadata": {
        "id": "IVX6iKpm-Qgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# once again, it is THAT simple to combine them\n",
        "model = Model(base_model.input, probabilities)"
      ],
      "metadata": {
        "id": "MftU9Jj26jV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.3. Freeze Pre-trained Layers:\n",
        "To preserve the knowledge embedded in the pre-trained model, we freeze its layers: we prevent them from being updated during training. At first, we will only update the last layer which we added ourself and that, for now, has random weights."
      ],
      "metadata": {
        "id": "ZwkeqHCt-8Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "d6Jkk8eH-3gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see what changed compared to base_model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BGaGezbJ_bhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Look at the number number non-trainable and trainable parameters*"
      ],
      "metadata": {
        "id": "1LGHVvvg_flU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wmutsG627Cx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32, epochs=10,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "oqYZzB-s9Ys_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####6.4. Fine-tune the model\n",
        "For further refinement, we unlock all layers in the pre-trained model and fine-tune the entire network with a small learning rate and high batch size."
      ],
      "metadata": {
        "id": "m6xSui2CAZ8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True"
      ],
      "metadata": {
        "id": "-M2I1mpxI1bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that all the parameters are now trainable"
      ],
      "metadata": {
        "id": "dDR0jrF9AlKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we set a low learning rate to avoid unlearning what the model has learnt\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dslmf9nLI2B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise**: Fine-tune the model - do you manage to outperform the previous CNN model?"
      ],
      "metadata": {
        "id": "Ex-a3ZtRKKhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "Bb_yXnidLCTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Optional content: Image augmentation"
      ],
      "metadata": {
        "id": "DBJAM_oyC9V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage both models are clearly overfitting the data, it is common for neural networks as they are extremely complex. We could reduce this effect by using *data augmentation*, artificially increase the number of images in our training dataset.\n",
        "\n",
        "`Albumentations` is great user-friendly library to perform various type of transformation on your images: https://albumentations.ai/, but we can also use Keras's `ImageDataGenerator`"
      ],
      "metadata": {
        "id": "CT7IEt2-C34u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Introduction to image augmentation"
      ],
      "metadata": {
        "id": "rL1Bm1mBJyW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation function\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define random transformations\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "SuZQXPWYEOXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose an image as an example\n",
        "n_image = np.random.randint(len(X_test))\n",
        "img = np.expand_dims(X_test[n_image], axis=0)"
      ],
      "metadata": {
        "id": "vNGT7SpFEgzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 3 augmented images\n",
        "augmented_images = []\n",
        "for batch in datagen.flow(img, batch_size=1):\n",
        "    augmented_images.append(batch[0])\n",
        "    if len(augmented_images) >= 3:\n",
        "        break"
      ],
      "metadata": {
        "id": "KFE4B4ReERbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 3 by 3 grid for visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(8, 3))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Display each augmented image\n",
        "for img, ax in zip(augmented_images, axes):\n",
        "    ax.imshow(img.squeeze())  # Assuming img is in (height, width, channels) format\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qPK564-VE38F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise**: training a model using a `ImageDataGenerator`"
      ],
      "metadata": {
        "id": "UN35uPrNJ22L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "8utlvZebJ7PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Saliency Maps: Interpreting Model Predictions"
      ],
      "metadata": {
        "id": "udSlW8KGB4Gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the realm of neural networks, particularly in image classification tasks, the generation of saliency maps provides valuable insights into the model's decision-making process. A saliency map highlights the regions in an input image that significantly contribute to the model's prediction for a specific class. The concept revolves around the fundamental question: \"For each input pixel, how much would the prediction change if this pixel was altered?\"\n",
        "\n",
        "#### **Practical Implementation**:\n",
        "\n",
        "The saliency map generation involves the following steps:\n",
        "\n",
        "**Model Prediction:** Obtain the model's predicted class for the input image.\n",
        "\n",
        "**Gradient Calculation:** Employ backpropagation to compute the gradient of the loss with respect to the input pixels.\n",
        "\n",
        "**Absolute Values and Normalization:** Take the absolute values of the gradients and normalize them to ensure consistency.\n",
        "\n",
        "**Visualization:** Overlay the normalized gradient values on the original image, producing a visual representation of pixel importance.\n",
        "\n",
        "Saliency maps offer a powerful means to interpret and validate neural network decisions, fostering transparency in the decision-making process and aiding model debugging and improvement.\n"
      ],
      "metadata": {
        "id": "m1FA8-r6NCtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose an image randomly in the dataset\n",
        "n_image = np.random.randint(len(X_test))\n",
        "image = np.expand_dims(X_test[n_image], axis=0)"
      ],
      "metadata": {
        "id": "B8P-q4GJLm2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model's predicted class index for this image\n",
        "predicted_class = np.argmax(model.predict(image)[0])\n",
        "print(f'Predicted class: {class_names[predicted_class]}')"
      ],
      "metadata": {
        "id": "w74zVYawMATZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following block is quite technical, it is deriving the gradient of the predicted score with respect to the input image: `tape.gradient(predicted_score, input_tensor)`"
      ],
      "metadata": {
        "id": "V4qXPlzDO0f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate gradients with respect to the input image for the predicted class\n",
        "input_tensor = tf.convert_to_tensor(image)\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(input_tensor)\n",
        "    prediction = model(input_tensor)\n",
        "    predicted_score = prediction[0, predicted_class]\n",
        "gradients = tape.gradient(predicted_score, input_tensor).numpy()\n",
        "gradients.shape"
      ],
      "metadata": {
        "id": "_Vti6CpEB8Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the absolute values of gradients and normalize\n",
        "gradients = tf.abs(gradients)\n",
        "max_grad = tf.reduce_max(gradients)\n",
        "if max_grad != 0:\n",
        "    gradients /= max_grad\n",
        "\n",
        "# Convert gradients to a grayscale image\n",
        "grayscale = tf.image.rgb_to_grayscale(gradients)\n",
        "saliency_map = tf.squeeze(grayscale).numpy()"
      ],
      "metadata": {
        "id": "0JX-v8mHPZOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is then quite useful to smooth the resulting saliency map to highlight regions of the input image instead of individual pixels."
      ],
      "metadata": {
        "id": "SkoziH-YQcQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smoothed_saliency_map = gaussian_filter(saliency_map, sigma=2)"
      ],
      "metadata": {
        "id": "gt3O3EtnQBEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tada! We can now overlay the saliency map over the input image:"
      ],
      "metadata": {
        "id": "BOSaJw5pQzkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image[0])\n",
        "plt.axis('off')\n",
        "plt.title(f'Predicted Class: {class_names[predicted_class]}')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(image[0])\n",
        "plt.imshow(smoothed_saliency_map, cmap='inferno', alpha=0.5)\n",
        "plt.title('Saliency Map with Outline')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "LZKCNtJTPxjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can play with it, and change the example image (`n_image`) to see where the CNN model looks to make its predictions."
      ],
      "metadata": {
        "id": "J-2RSsejQ943"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **THE END**"
      ],
      "metadata": {
        "id": "e5MM25Q6Rb75"
      }
    }
  ]
}
