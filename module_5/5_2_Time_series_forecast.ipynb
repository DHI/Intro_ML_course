{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series forecasting\n",
    "\n",
    "Time series forecasting involves using historical time series data to predict future values of the series. Machine learning models offer several advantages for time series forecasting:\n",
    "* can hadle complex temporal patterns\n",
    "* can integrate multiple types/sources of data\n",
    "* can adapt to new trends over time\n",
    "* are easy to implement\n",
    "\n",
    "In this module we will extend the river discharge problem in the time dimension and try to predict the discharge in the next time step using the information available at the present time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load libraries and data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "file_url = 'https://github.com/DHI/Intro_ML_course/raw/main/module_5/danube_discharge.csv'\n",
    "df = pd.read_csv(file_url, parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature engineering**\n",
    "\n",
    "First of all, we need to transform the data in a format that provides the most information for predicting the river discharge. Given the temporal dynamics we expect that the aggregate temperature and precipitation values can be more informative than the daily ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['precip_weekly_sum'] = df['precipitation'].rolling(7).sum()\n",
    "df['precip_monthly_sum'] = df['precipitation'].rolling(30).sum()\n",
    "df['precip_quartelry_sum'] = df['precipitation'].rolling(120).mean()\n",
    "df['temperature_weekly_mean'] = df['temperature'].rolling(7).mean()\n",
    "df['temperature_monthly_mean'] = df['temperature'].rolling(30).mean()\n",
    "df['temperature_quarterly_mean'] = df['temperature'].rolling(120).mean()\n",
    "df['month_sin'] = np.sin(2*np.pi*(df.index.month)/12)\n",
    "df['month_cos'] = np.cos(2*np.pi*(df.index.month)/12)\n",
    "df['week_sin'] = np.sin(2*np.pi*(df.index.day_of_year/7)/52)\n",
    "df['week_cos'] = np.cos(2*np.pi*(df.index.day_of_year/7)/52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check how these features are correlated with the discharge to select the best predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the pairwise cross-correlation matrix of all columns\n",
    "cross_correlation_matrix = df.corr()\n",
    "\n",
    "cross_correlation_matrix.discharge.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use as input features all the variables with absolute correlation higher than 0.1, except for the discharge itself, and set as target the discharge at the following time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ANN = df[['discharge', 'precip_monthly_sum', 'precip_quartelry_sum',\n",
    "             'temperature_monthly_mean', 'temperature_quarterly_mean', \n",
    "             'month_sin', 'month_cos', 'week_sin', 'week_cos']].copy()\n",
    "df_ANN.columns = ['y_t', 'x1_t', 'x2_t', 'x3_t', 'x4_t', 'x5_t', 'x6_t', 'x7_t', 'x8_t']\n",
    "df_ANN['y_t+1'] = df_ANN['y_t'].shift(-1)\n",
    "\n",
    "# Reorder columns\n",
    "df_ANN = df_ANN[['x1_t', 'x2_t', 'x3_t', 'x4_t', 'x5_t', 'x6_t', 'x7_t', 'x8_t', 'y_t+1']] # Notice we have dropped y_t (discharge)\n",
    "\n",
    "# Remove initial and final rows with missing values\n",
    "df_ANN = df_ANN[~df_ANN.isnull().any(axis=1)]\n",
    "\n",
    "df_ANN.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ANN.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will separate input and target values, normalize the data and split it sequentially in training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ANN.iloc[:, :-1].values\n",
    "y = df_ANN.iloc[:, -1].values\n",
    "\n",
    "data_length = len(X)\n",
    "train_end = int(data_length * 0.7) # 70% of data is used for training\n",
    "validation_end = int(data_length * 0.85) # 15% of data is used for validation and test\n",
    "\n",
    "train_end, validation_end, data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:train_end]\n",
    "X_val = X[train_end:validation_end]\n",
    "X_test = X[validation_end:]\n",
    "\n",
    "X_train.shape, X_test.shape, X_val.shape\n",
    "\n",
    "y_train = y[:train_end]\n",
    "y_val = y[train_end:validation_end]\n",
    "y_test = y[validation_end:]\n",
    "\n",
    "y_train.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler_X = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "X_train_scaled.shape, X_val_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compile and train the MLP (ANN) with our choice of architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Building the ANN model\n",
    "model = Sequential([\n",
    "    Dense(units=16, activation='relu', input_dim=8),\n",
    "    Dropout(0.1), # Read about dropout layers here: https://keras.io/api/layers/regularization_layers/dropout/\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(units=1, activation='linear')\n",
    "])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='mean_squared_error')\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train_scaled, y_train_scaled, \n",
    "                    batch_size=2, epochs=50, \n",
    "                    validation_data=(X_val_scaled, y_val_scaled), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of the model we will plot the learning curve, compute the RMSE on the test predictions and visualize the predictions against the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.title('Learning curves')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions for the test dataset\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "predictions = y_pred.reshape(-1)\n",
    "\n",
    "# Print MSE score of test predictions\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print('RMSE (test): ', rmse)\n",
    "\n",
    "# Plot the predictions against the actual values\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_ANN.index[validation_end:], y=y_test, name='Observed', line=dict(color='black')))\n",
    "fig.add_trace(go.Scatter(x=df_ANN.index[validation_end:], y=predictions, name='Predicted (test)', line=dict(color='red')))\n",
    "fig.update_yaxes(title_text=\"Discharge (m3/s)\")\n",
    "fig.update_xaxes(title_text=\"Day\")\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on the same plot the prediction for the train, validation and test datasets\n",
    "y_train_pred_scaled = model.predict(X_train_scaled)\n",
    "y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled)\n",
    "predictions_train = y_train_pred.reshape(-1)\n",
    "\n",
    "y_val_pred_scaled = model.predict(X_val_scaled)\n",
    "y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled)\n",
    "predictions_val = y_val_pred.reshape(-1)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_ANN.index, y=df_ANN['y_t+1'], name='Observed', line=dict(color='black')))\n",
    "fig.add_trace(go.Scatter(x=df_ANN.index[:train_end], y=predictions_train, name='Predictions (train)', line=dict(color='green')))\n",
    "fig.add_trace(go.Scatter(x=df_ANN.index[train_end:validation_end], y=predictions_val, name='Predictions (validation)', line=dict(color='violet')))\n",
    "fig.add_trace(go.Scatter(x=df_ANN.index[validation_end:], y=predictions, name='Predictions (test)', line=dict(color='red')))\n",
    "fig.update_yaxes(title_text=\"Discharge (m3/s)\")\n",
    "fig.update_xaxes(title_text=\"Day\")\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
