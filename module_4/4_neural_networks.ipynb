{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4 - Introduction to Neural Networks: Training a Multi-Layer Perceptron\n"
      ],
      "metadata": {
        "id": "BQ1PHaOBCueJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/DHI/Intro_ML_course/blob/main/module_4/4_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "1Z4p6HSIEaUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will delve into the fascinating world of neural networks, particularly focusing on training a Multi-Layer Perceptron (MLP) for image classification. Our objective is to utilize the renowned CIFAR-10 dataset, converted to black and white, to introduce you to the fundamentals of building and training neural networks.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "1. Implement an architecture using the `Sequential` model in Keras.\n",
        "2. Compile the model with suitable parameters for efficient training.\n",
        "3. Utilize various callbacks such as `ReduceLROnPlateau` and `EarlyStopping` to enhance training performance.\n",
        "4. Study the impact of different batch sizes on the training speed and overall performance of the model.\n",
        "5. Visualize the results, including model predictions, to gain insights into the learning process and performance metrics.\n",
        "\n",
        "Throughout this notebook, we will provide step-by-step instructions, code demonstrations, and explanations to help you understand the key concepts and methodologies involved in training neural networks for image classification tasks. By the end of this tutorial, you will have a solid understanding of how to build and train a basic neural network for image classification tasks, along with valuable insights into best practices and performance optimization techniques.\n",
        "\n",
        "Let's embark on this exciting journey into the realm of neural networks and image classification!"
      ],
      "metadata": {
        "id": "QdgQdshgGGb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "RVtDqAHLL_I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load the CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "sNoAfEHa-bV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into building and training the Multi-Layer Perceptron (MLP) model, we need to load the CIFAR-10 dataset. CIFAR-10 is a popular image classification dataset that consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. Created by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton, this dataset serves as a benchmark for training and evaluating various image classification models. In this notebook, we will convert the images to grayscale for simplicity.\n",
        "\n",
        "The dataset is conveniently available through the `tf.keras.datasets.cifar10` module, which allows us to easily load the dataset for training and testing our neural network. The loaded data will be split into training and testing sets, consisting of images and corresponding labels that we will use to train and evaluate our MLP model.\n",
        "\n",
        "Let's load the CIFAR-10 dataset and get ready to explore the fascinating world of neural networks and image classification!\n"
      ],
      "metadata": {
        "id": "BK7bdHq4Gbc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "YN_TVxeANx1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 60,000 images of 32 by 32 pixels - 50,000 for training and validation and 10,000 for testing. The images are in color so each pixel is defined by its red, green and blue level."
      ],
      "metadata": {
        "id": "xSNNejzHHWUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "id": "VTByU9IpHTtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels are integers between 0 and 9:"
      ],
      "metadata": {
        "id": "GPi9yntSIqIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels)"
      ],
      "metadata": {
        "id": "op9YSvXnHSLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the class names corresponding to the CIFAR-10 dataset to establish a relationship between the previous integers and the classes.\n",
        "- 0: `airplane`\n",
        "- 1: `automobile`\n",
        "- 2: `bird`\n",
        "- 3: `cat`\n",
        "- 4: `deer`\n",
        "- 5: `dog`\n",
        "- 6: `frog`\n",
        "- 7: `horse`\n",
        "- 8: `ship`\n",
        "- 9: `truck`"
      ],
      "metadata": {
        "id": "skpbOK2WJLdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class names for CIFAR-10\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "S2dn5MpcOMAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some sample images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i])\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A62f5rxfN2Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pre-process the data"
      ],
      "metadata": {
        "id": "epow3FIQ-Yny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training our neural network, we pre-process the CIFAR-10 images to one-dimensional vector, as we saw in the video:\n",
        "1.   **Conversion to grayscale**: We average the RGB values to obtain a single intensity\n",
        "2.   **Flattening**: We transform the $32 \\times 32$ pixels images into a one-dimensional vector, where each element represents the intensity value of a single pixel.\n",
        "3.  **Scaling**: Convert the intensities from integers in the range 0-255 to a `float` value between 0 and 1."
      ],
      "metadata": {
        "id": "CW9olGCxKyqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_images[0, :, :, 0]) # red band of first image"
      ],
      "metadata": {
        "id": "f-rqmYTtNa_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of dataset before pre-processing', train_images.shape)\n",
        "\n",
        "# Convert to grayscale\n",
        "train_data = train_images.mean(axis=-1)\n",
        "test_data = test_images.mean(axis=-1)\n",
        "print('Shape of dataset after grayscale', train_data.shape)\n",
        "\n",
        "# Flatten images - convert to a 1d vector\n",
        "train_data = train_data.reshape((50000, 32*32))\n",
        "test_data = test_data.reshape((10000, 32*32))\n",
        "print('Shape of dataset after flattening', train_data.shape)\n",
        "\n",
        "# Scaling - from 0-255 to 0-1\n",
        "train_data = train_data/255.\n",
        "test_data = test_data/255."
      ],
      "metadata": {
        "id": "M-QFq_iDUscr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0]) # 1d vector corresponding to first image"
      ],
      "metadata": {
        "id": "8PbAUVp5NZwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattened, the images look like this:"
      ],
      "metadata": {
        "id": "npn6m4MGNwhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.imshow(train_data[0].reshape(1, -1), cmap='gray')\n",
        "ax.set_aspect(100)\n",
        "ax.set_title('This is a ' + class_names[train_labels[0][0]])"
      ],
      "metadata": {
        "id": "LiwUsKwGN0r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Build a multi-layer perceptron (MLP)"
      ],
      "metadata": {
        "id": "QVGX1D_HPB42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We construct a Multi-Layer Perceptron (MLP) model using the Keras `Sequential` API. The model architecture consists of the following layers:\n",
        "\n",
        "1. **Input Layer:** The input shape is explicitly defined to accept the flattened black and white images.\n",
        "\n",
        "2. **Dense Layers:** We incorporate two hidden dense layers with 128 and 64 neurons, respectively, using `ReLU` (Rectified Linear Unit) activation functions. These layers enable the network to learn complex patterns and representations from the flattened input data.\n",
        "\n",
        "3. **Output Layer:** The final dense layer consists of 10 neurons, representing the probabilties of each of the 10 classes in the CIFAR-10 dataset. The `SoftMax` activation function converts raw values to probabilties (values between 0 and 1 which sum is equal to 1).\n",
        "\n",
        "By constructing this MLP model, we aim to leverage its capacity to learn and classify the pre-processed image data accurately. Let's proceed with compiling the model and configuring the necessary parameters for training.\n"
      ],
      "metadata": {
        "id": "tVDu8loiPUHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(1024,)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "QcAdygu3N5tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "88IBmP9Lvwwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After constructing the layers of our Multi-Layer Perceptron (MLP) model, we need to compile it to configure the learning process. The `compile` method allows us to define the optimizer, loss function, and evaluation metrics for training the model. In this case, we set the following configurations:\n",
        "\n",
        "1. **Optimizer:** We utilize the widely-used `Adam` optimizer which is gradient descent method.\n",
        "\n",
        "2. **Learning Rate:** The learning rate parameter controls the step size during optimization and influences the speed and quality of convergence during training.\n",
        "\n",
        "3. **Loss Function:** For the multi-class classification task, we employ the `SparseCategoricalCrossentropy` loss function, which is well-suited for scenarios where the target labels are provided as integers.\n",
        "\n",
        "4. **Metrics:** We track the 'accuracy' metric during training to monitor the performance of the model and assess its classification accuracy."
      ],
      "metadata": {
        "id": "si3ojl7IRqWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3VwTGcyWPyIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train the model"
      ],
      "metadata": {
        "id": "FMr_susNY3X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To train the Multi-Layer Perceptron (MLP) model on the pre-processed CIFAR-10 dataset, we utilize the magical function `model.fit` function. We need to provide the following:\n",
        "\n",
        "1. **Training Data:** The pre-processed training data is provided to the model for learning patterns and features associated with the input images.\n",
        "\n",
        "2. **Training Labels:** The corresponding labels for the training data are provided to guide the model during the learning process.\n",
        "\n",
        "3. **Batch Size:** We set the batch size to 128, determining the number of samples used in each iteration for updating the model's weights.\n",
        "\n",
        "4. **Epochs:** The model is trained for 10 epochs, representing the number of times the entire dataset is passed forward and backward through the neural network.\n",
        "\n",
        "5. **Validation Data:** We evaluate the model's performance on the provided test dataset (`test_data` and `test_labels`) to monitor its generalization ability during training.\n"
      ],
      "metadata": {
        "id": "umU2bsqSYmlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, train_labels, batch_size=64, epochs=20, validation_data=(test_data, test_labels))"
      ],
      "metadata": {
        "id": "FDIO6XvSWYoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.title('Learning curves')"
      ],
      "metadata": {
        "id": "FU7TYQw0XdjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the model is in the process of learning, but after 10 epochs, it's possible that the model has not fully converged. To ensure that the model has sufficient time to learn the underlying patterns in the data, we will continue the learning but, this time, with an *early stopping callback*.\n",
        "This technique allows us to continue training the model until the validation loss stabilizes, indicating that the model has reached an optimal point of convergence. By incorporating the early stopping mechanism, we can prevent overfitting and ensure that the model generalizes well to unseen data."
      ],
      "metadata": {
        "id": "hkAsghTpYUj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Training the model with the early stopping callback\n",
        "history = model.fit(train_data, train_labels,\n",
        "                    batch_size=128,\n",
        "                    epochs=100,\n",
        "                    validation_data=(test_data, test_labels),\n",
        "                    callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "jX7qLyLzZjTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Here, `patience=3` implies that training will stop after 3 epochs with no improvement in the validation loss. Setting `restore_best_weights=True` ensures that the model's weights are restored to the best achieved state, as determined by the minimum loss on the test data, at the end of training."
      ],
      "metadata": {
        "id": "H5A309VhaKoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.title('Learning curves - part 2')"
      ],
      "metadata": {
        "id": "0m7HfnWcp7uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reminder**: https://playground.tensorflow.org/ is a great resource to get an understanding of neural networks"
      ],
      "metadata": {
        "id": "soRCd2m795FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluate the model"
      ],
      "metadata": {
        "id": "ynu17mR5aYX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will evaluate the performance of our trained Multi-Layer Perceptron (MLP) model for image classification on the test dataset.\n",
        "\n",
        "First, let's output the probabilities (one for each of the ten classes) of the 10,000 test images:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tP1Fzwlszy8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_data)\n",
        "predictions.shape"
      ],
      "metadata": {
        "id": "hAbaVxuwaxSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the following block multiple times to check the results on various images**"
      ],
      "metadata": {
        "id": "exNV2bkIerLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick one instance in the test dataset\n",
        "random_index = np.random.randint(len(test_data))\n",
        "\n",
        "# display probabilities\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].bar(class_names, predictions[random_index], color='skyblue')\n",
        "ax[0].set_title(f'Predicted probabilities for image {random_index} of test dataset')\n",
        "ax[0].set_ylabel('Probabilities')\n",
        "ax[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
        "\n",
        "# show image\n",
        "ax[1].imshow(test_images[random_index])"
      ],
      "metadata": {
        "id": "zb1B65rKbG_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assign each instance to the class with maximal probability:"
      ],
      "metadata": {
        "id": "-dYyeBr_0ajC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from probabilities to predict\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "predicted_classes.shape"
      ],
      "metadata": {
        "id": "X42h_zGXdKbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix helps us identify pairs like 'dog' and 'cat,' 'automobile' and 'truck,' and 'airplane' and 'ship,' which the model might have difficulties telling apart due to their visual similarities."
      ],
      "metadata": {
        "id": "lSNOdS_z0sIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display confusion matrix\n",
        "cm = confusion_matrix(test_labels, predicted_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "elhjNQPcdIfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise - derive the accuracy score on the test dataset"
      ],
      "metadata": {
        "id": "KTIyixjx7Fz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Learning rate"
      ],
      "metadata": {
        "id": "SzOVNnXafENC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Understanding the impact of the learning rate on the training process is crucial for optimizing model performance. In this module, we'll see how the learning rate influences the speed of model convergence and the overall efficiency of the training process. By exploring the relationship between the learning rate and loss function, we'll try to identify the optimal learning rate for our Multi-Layer Perceptron (MLP) model on the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "1FcNiVG_ACva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function which defines, compiles and trains a model for values of lr and batch size\n",
        "def train_model_with_learning_rate(learning_rate, batch_size):\n",
        "    model = keras.Sequential([layers.Input(shape=(1024,)),\n",
        "                              layers.Dense(128, activation='relu'),\n",
        "                              layers.Dense(64, activation='relu'),\n",
        "                              layers.Dense(10, activation='softmax')])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_data, train_labels, batch_size=batch_size, epochs=10,\n",
        "                        validation_data=(test_data, test_labels))\n",
        "    return history"
      ],
      "metadata": {
        "id": "t0y4q9uGPOcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looping over the different learning rate, we can compare their learning curves.\n",
        "\n",
        "**Note**: running the following block can be quite long (a few minutes)..."
      ],
      "metadata": {
        "id": "YWYoupDuAYAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = []\n",
        "for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
        "  print(f'-------------- Learning rate = {lr} --------------')\n",
        "  history = train_model_with_learning_rate(learning_rate=lr, batch_size=128)\n",
        "  histories.append(history)"
      ],
      "metadata": {
        "id": "rhYO9z0F7Rd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the learning curves (validation loss)\n",
        "for i, lr in enumerate([0.00001, 0.0001, 0.001, 0.01, 0.1]):\n",
        "  plt.plot(histories[i].history['val_loss'], label='LR = ' + str(lr))\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "f7wS5Es6Rj0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Exercise - Influence of the batch size on the training speed"
      ],
      "metadata": {
        "id": "PriGu3r67okZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: By training the model with various values of batch size (for instance: 4, 16, 32, 256, 1024), find out how this hyperparameter influences the learning curves.\n",
        "\n",
        "**Note**: Training a model on multiple epochs with a low number of batch size (e.g. 4) can be very slow due to the increased frequency of weight updates. Increasing the batch size allows for more efficient parallel processing, and a too high batch size might lead to a memory overload."
      ],
      "metadata": {
        "id": "-BIC72yJBGOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Exercise - do colors matter?"
      ],
      "metadata": {
        "id": "uIv_EsF_9bLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Train a model which, instead of using grayscale images, uses the color image. Does this model outperform the previous models which used grayscale images, i.e. do colors help the model to interpret images?\n",
        "\n",
        "**Hint**: You can consider converting each image - of original shape $(32, 32, 3)$ - to a one-dimensional vector of shape $(3072, )$"
      ],
      "metadata": {
        "id": "OVSWK-v7HyeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing\n",
        "print('Shape of dataset before pre-processing', train_images.shape)\n",
        "\n",
        "# Flatten train_images and test_images - convert to a 1d vector\n",
        "...\n",
        "\n",
        "# Scaling - from 0-255 to 0-1\n",
        "..."
      ],
      "metadata": {
        "id": "Al8SLpbs9Bxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model architecture\n",
        "model = ..."
      ],
      "metadata": {
        "id": "QOcmXfBH9KVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer=...,\n",
        "              loss...,\n",
        "              metrics=...)"
      ],
      "metadata": {
        "id": "_HStzvKH9jn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(...)"
      ],
      "metadata": {
        "id": "SzPrYrFq9BZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "ub52dMAV_P4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy score on the test dataset?"
      ],
      "metadata": {
        "id": "9MqZX-B15_gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Optional content - Keras callback: Reduce learning rate on plateau"
      ],
      "metadata": {
        "id": "6Q8f3ku9dBvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras offers powerful callbacks to monitor and adjust the model during training. We'll focus on two key callbacks:\n",
        "\n",
        "- **ReduceLROnPlateau:** Adjusts the learning rate during plateaus for stable convergence. It uses high learning rate at the start of the training and gradually reduces it.\n",
        "- **EarlyStopping:** Halts training when validation loss stagnates, preventing overfitting. We already used it in section 4.\n",
        "\n",
        "Implementing these callbacks enhances our MLP model's learning and performance on the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "PLgEtUmy1qHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the ReduceLROnPlateau callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                 factor=0.5, patience=4, min_lr=0.000001)\n",
        "\n",
        "# Defining the EarlyStopping callback\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              patience=7, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "rJYHQI3jEiim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-initialize and compile the model, this time with a larger learning rate (0.01 instead of 0.001)"
      ],
      "metadata": {
        "id": "CabDBYAQ7FFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([layers.Input(shape=(1024,)),\n",
        "                              layers.Dense(128, activation='relu'),\n",
        "                              layers.Dense(64, activation='relu'),\n",
        "                              layers.Dense(10, activation='softmax')])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5nbS7gf_3AYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the callbacks to the model training\n",
        "history = model.fit(train_data, train_labels,\n",
        "                    batch_size=128,\n",
        "                    epochs=100,\n",
        "                    validation_data=(test_data, test_labels),\n",
        "                    callbacks=[reduce_lr, early_stop])"
      ],
      "metadata": {
        "id": "BPykeVzE2eFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.title('Learning curves')"
      ],
      "metadata": {
        "id": "w-28n5kb372M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THE END**"
      ],
      "metadata": {
        "id": "3vJGx1H7Djk0"
      }
    }
  ]
}
