{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression models\n",
    "\n",
    "Regression models are a type of supervised models that predict a continuous output variable, known as the dependent or target variable, based on one or more input variables, known as independent or predictor variables. The goal of regression is to find the relationship or mapping between the input(s) and the output, and use it to make predictions for new input data.\n",
    "\n",
    "In this module we will start from the most common types of regression models: linear and polynomial. As we increase the complexity of the model (number of parameters), we also increase the risk of overfitting the training data. We will check for overfitting using learning curves and then show which  regularization techniques can reduce overfitting: Ridge, Lasso and ElasticNet. Finally we will explore the use for RandomForest models for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Load libraries and data**\n",
    "\n",
    "In this module we will continue using the large_rivers dataset that was introduced in Module 1. Please, refer to that if your need more details explanations on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe from xlsx file\n",
    "file_url = 'https://github.com/DHI/Intro_ML_course/raw/main/module_1/large_rivers_processed.csv'\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "# If you are unable to read the file from the url, you can download it and read it locally\n",
    "# file_path = 'large_rivers_processed.csv'\n",
    "# df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Linear regression**\n",
    "\n",
    "Let's start from a univariate problem, where we predict the Discharge given the catchmet Area. We expect a log-transformation to increase the performance of the model as it will reduce the impact of outliers, by compressing the range of the values. This will also facilitate the visualization of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable y\n",
    "y = df['Discharge']\n",
    "\n",
    "# Select input features X (only Area)\n",
    "X = df[['Area']]\n",
    "\n",
    "# Apply log transformation to both X and y\n",
    "X = np.log(X)\n",
    "y = np.log(y)\n",
    "\n",
    "# Plot log transformed data\n",
    "plt.scatter(X, y, marker='.')\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# random_state is an arbitrary number (e.g., 42) ensuring reproducibility.\n",
    "\n",
    "# Print shape of all subsets\n",
    "print('Shape of X_train: ', X_train.shape)\n",
    "print('Shape of y_train: ', y_train.shape)\n",
    "print('Shape of X_test: ', X_test.shape)\n",
    "print('Shape of y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and test sets in the same scatter plot\n",
    "plt.scatter(X_train, y_train, marker='.', label='Training set')\n",
    "plt.scatter(X_test, y_test, marker='.', label='Test set')\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare histograms of training and test sets\n",
    "plt.hist(y_train, bins=20, label='Training set')\n",
    "plt.hist(y_test, bins=20, label='Test set')\n",
    "plt.xlabel('log(Discharge)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by fitting a linear model (straight line) to the data. For this univariate problem, the model coefficients will be equivalent to intercept and slope of the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and visualized the parameters of the trained model\n",
    "slope = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(f\"Slope: {slope}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "\n",
    "# Plot scatter of x_train vs y_train and regression line\n",
    "plt.scatter(X_train, y_train, label='Training data')\n",
    "plt.plot(X_train, slope*X_train + intercept, color='red', label='Regression line')\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to evaluate the predictions on a independent test set. We can do this by using the predict method of the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the mean squared error of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "plt.scatter(X_test, y_test, label='Test data')\n",
    "plt.scatter(X_test, y_pred, label='Predicted data')\n",
    "# Add vertical lines connecting points to regression line\n",
    "for i in range(len(X_test)):\n",
    "    plt.plot([X_test.iloc[i,0], X_test.iloc[i,0]], [y_test.iloc[i],y_pred[i]], color='orange', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.plot([], [], color='orange', linestyle='--', linewidth=1, label='Prediction error') # Add entry legend\n",
    "\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "\n",
    "Reverse the log-transform of the model predictions and compute the prediction error for the test data as RMSE in the original measuring unit (m3/s). Which other error metric can be relevant? See pre-defined metrics in sklearn: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Polynomial regression**\n",
    "\n",
    "Polynomial transformations of the input features allow to use a linear model to fit nonlinear data. All we need to do is adding powers of each feature as new features, and train a linear model on this extended set of features. The degree of the polynomial is the higher power we use in the transformation and therefore determines the number of parameters (degree of) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize polynomial features object\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform X_train and X_test\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "#Compare first instance in original and transformed X_train\n",
    "print('Original: ', X_train.iloc[0,:])\n",
    "print('Transformed: ', X_train_poly[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear model on the polynomial features\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "# Compute the mean squared error of the model\n",
    "mse = mean_squared_error(y_test, y_pred_poly)\n",
    "print('MSE: ', mse)\n",
    "\n",
    "# Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "plt.scatter(X_test, y_test, label='Test data')\n",
    "\n",
    "# Sort X_test and y_pred by X_test\n",
    "idx = np.argsort(X_test.values.flatten())\n",
    "X_test_sorted = X_test.values[idx]\n",
    "y_pred_poly_sorted = y_pred_poly[idx]\n",
    "\n",
    "plt.plot(X_test_sorted, y_pred_poly_sorted, color='red', label='Predicted data')\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot polynomial regression with different degrees\n",
    "\n",
    "def polynomial_regression(degree):\n",
    "    # Initialize polynomial features object\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Transform X_train and X_test\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "    # Fit linear model on the polynomial features\n",
    "    model_poly = LinearRegression()\n",
    "    model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    y_pred_poly_train = model_poly.predict(X_train_poly)\n",
    "\n",
    "    # Compute the mean squared error of the model\n",
    "    mse_train = mean_squared_error(y_train, y_pred_poly_train)\n",
    "\n",
    "    # Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "    plt.scatter(X_train, y_train, label='Training data')\n",
    "    \n",
    "    # Plot polynomial line\n",
    "    x_range = np.linspace(X_train.min(), X_train.max(), 100)\n",
    "    x_range_poly = poly.fit_transform(x_range.reshape(-1, 1))\n",
    "    y_range_poly = model_poly.predict(x_range_poly)\n",
    "    plt.plot(x_range, y_range_poly, color='red', label='Polynomial regression')\n",
    "    \n",
    "    plt.xlabel('log(Area)')\n",
    "    plt.ylabel('log(Discharge)')\n",
    "    # Plot MSe in title\n",
    "    plt.title(f'MSE on training set (degree={degree}): {mse_train:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_poly_test = model_poly.predict(X_test_poly)\n",
    "\n",
    "    # Compute the mean squared error of the model\n",
    "    mse_test = mean_squared_error(y_test, y_pred_poly_test)\n",
    "\n",
    "    # Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "    plt.scatter(X_test, y_test, label='Test data')\n",
    "    \n",
    "    # Plot polynomial line\n",
    "    x_range = np.linspace(X_test.min(), X_test.max(), 100)\n",
    "    x_range_poly = poly.fit_transform(x_range.reshape(-1, 1))\n",
    "    y_range_poly = model_poly.predict(x_range_poly)\n",
    "    plt.plot(x_range, y_range_poly, color='red', label='Polynomial prediction')\n",
    "    \n",
    "    plt.xlabel('log(Area)')\n",
    "    plt.ylabel('log(Discharge)')\n",
    "    # Plot MSe in title\n",
    "    plt.title(f'MSE on test set (degree={degree}): {mse_test:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "# Call function for different degrees\n",
    "polynomial_regression(2)\n",
    "polynomial_regression(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Bias/variance tradeoff**\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning and statistics that describes the tradeoff between two sources of error that affect the performance of a predictive model:\n",
    "* **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias means that the model is too simplistic and can't capture the underlying patterns in the data, leading to systematic errors regardless of the dataset used for training. This is also known as underfitting.\n",
    "\n",
    "* **Variance** refers to the model's sensitivity to small changes in the training data. High variance means the model is highly flexible and fits the training data very closely, including the noise. As a result, it may perform poorly on new, unseen data because it's too tailored to the training set. This is known as overfitting.\n",
    "\n",
    "Ideally, we would want a model with low bias and low variance, but in practice, there is a **tradeoff**. As we increase model complexity (e.g., more parameters), variance increases and bias decreases. Conversely, as we decrease model complexity, bias increases and variance decreases. The goal is to find the sweet spot where the total error is minimized, which usually involves balancing a moderate amount of bias and variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(degree, X, y):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.fit_transform(X_test)\n",
    "    model_poly = LinearRegression()\n",
    "    model_poly.fit(X_train_poly, y_train)\n",
    "    train_errors, test_errors = [], []\n",
    "    for m in range(1, len(X_train_poly)):\n",
    "        model.fit(X_train_poly[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train_poly[:m])\n",
    "        y_test_predict = model.predict(X_test_poly)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        test_errors.append(mean_squared_error(y_test, y_test_predict))\n",
    "    plt.plot(train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(test_errors, \"b-\", linewidth=3, label=\"test\")\n",
    "    plt.ylim(0,1.5)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title(f'Learning curves of polynomial model (degree={degree})')\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(2, X, y)\n",
    "plot_learning_curves(20, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Which polynomial degree offers the best tradeoff between the bias and variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Regularized linear models**\n",
    "\n",
    "As shown with decision trees, a good way to reduce overfitting is to constrain the model: the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For polynomial models, the degrees of freedom can be limited by reducing the degree. This process is also called \"regularization\".\n",
    "\n",
    "For a linear model, regularization can also be achieved by constraining the weights of the model. In the following we will look at three different regularization techiques that constrain the model weights: Ridge, Lasso and Elastic Net.\n",
    "\n",
    "Before applying these regularization techniques it is generally recommended to normalize the data, otherwise some features would be unfairly penalized based on the scale of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply min/max normalization to X_train and X_test \n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same scaling parameters to X_test\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression** (L2 regulatization)\n",
    "* Cost: mse + aplha * the sum of the squared model weights\n",
    "* Tends to shrink the coefficients towards zero but does not set any of them exactly to zero.\n",
    "* Useful when there are many features, all of which contribute to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ridge regression to polynomial regression of degree 20 with different alpha value equal to 1 and plot regression line\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize polynomial features object\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform X_train and X_test\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.fit_transform(X_test_scaled)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "plt.scatter(X_test, y_test, label='Test data')\n",
    "\n",
    "def plot_ridge_regression(alpha):\n",
    "    # Fit linear model on the polynomial features\n",
    "    model_poly = Ridge(alpha=alpha, solver=\"cholesky\")\n",
    "    model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "    # Sort X_test and y_pred by X_test\n",
    "    idx = np.argsort(X_test.values.flatten())\n",
    "    X_test_sorted = X_test.values[idx]\n",
    "    y_pred_poly_sorted = y_pred_poly[idx]\n",
    "\n",
    "    plt.plot(X_test_sorted, y_pred_poly_sorted, label='Prediction, alpha = {}'.format(alpha))\n",
    "  \n",
    "\n",
    "plot_ridge_regression(0)\n",
    "plot_ridge_regression(10)\n",
    "plot_ridge_regression(100)\n",
    "\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso regression** (L1 regulatization)\n",
    "* Cost: a mix of the cost functions of Ridge and Lasso\n",
    "* Tends to shrink some coefficients exactly to zero, effectively performing feature selection\n",
    "* Useful when there are many features but only a few are expected to contribute to the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lasso regression to polynomial regression of degree 20 with different alpha value equal to 1 and plot regression line\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize polynomial features object\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform X_train and X_test\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.fit_transform(X_test_scaled)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "plt.scatter(X_test, y_test, label='Test data')\n",
    "\n",
    "def plot_lasso_regression(alpha):\n",
    "    # Fit linear model on the polynomial features\n",
    "    model_poly = Lasso(alpha=alpha)\n",
    "    model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "    # Sort X_test and y_pred by X_test\n",
    "    idx = np.argsort(X_test.values.flatten())\n",
    "    X_test_sorted = X_test.values[idx]\n",
    "    y_pred_poly_sorted = y_pred_poly[idx]\n",
    "\n",
    "    plt.plot(X_test_sorted, y_pred_poly_sorted, label='Prediction, alpha = {}'.format(alpha))\n",
    "\n",
    "plot_lasso_regression(0)\n",
    "plot_lasso_regression(0.08)\n",
    "plot_lasso_regression(1)\n",
    "\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic Net regression**\n",
    "* Cost: mix of cost functions from Ridge and Lasso\n",
    "* Can perform feature selection like Lasso while still retaining Ridge's benefits of coefficient shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ElasticNet regression to polynomial regression of degree 20 with different alpha value equal to 1 and plot regression line\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Initialize polynomial features object\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Transform X_train and X_test\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.fit_transform(X_test_scaled)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "plt.scatter(X_test, y_test, label='Test data')\n",
    "\n",
    "def plot_elasticnet_regression(alpha):\n",
    "    # Fit linear model on the polynomial features\n",
    "    model_poly = ElasticNet(alpha=alpha, l1_ratio=0.5)\n",
    "    model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "    # Sort X_test and y_pred by X_test\n",
    "    idx = np.argsort(X_test.values.flatten())\n",
    "    X_test_sorted = X_test.values[idx]\n",
    "    y_pred_poly_sorted = y_pred_poly[idx]\n",
    "\n",
    "    plt.plot(X_test_sorted, y_pred_poly_sorted, label='Prediction, alpha = {}'.format(alpha))\n",
    "\n",
    "plot_elasticnet_regression(0)\n",
    "plot_elasticnet_regression(0.1)\n",
    "plot_elasticnet_regression(1)\n",
    "\n",
    "plt.xlabel('log(Area)')\n",
    "plt.ylabel('log(Discharge)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Can the polynomial regression of degree 20 outperform the best model found in Exercise 2 with the help of regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6 Multivariate linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply linear model to predict discharge from all the other features\n",
    "\n",
    "# Define target variable y\n",
    "y = df['Discharge']\n",
    "y = np.log(y)\n",
    "\n",
    "# Select input features X (all except Discharge)\n",
    "X = df.drop(['Discharge', 'Name', 'Continent'], axis=1)\n",
    "\n",
    "# Apply log transformation to Area only\n",
    "X['Area'] = np.log(X['Area'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply min/max normalization to X_train and X_test\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "# Compute the mean squared error of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True discharge (log)')\n",
    "plt.ylabel('Predicted discharge (log)')\n",
    "# Add mse score in title\n",
    "plt.title(f'MSE: {mse:.2f}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print bar chart with value of coefficients\n",
    "fig, ax = plt.subplots(figsize=(5,2))\n",
    "plt.bar(X.columns, lin_reg.coef_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Apply the regularization techniques shown above. How do the trained coefficients of the linear model change? Can you use regularization to effectively discard some features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7 Random forest regressor** (optional)\n",
    "\n",
    "Random Forest constructs multiple decision trees using different subsets of the training data, both in terms of instances and features. The final prediction is then obtained by aggregating the predictions from the individual trees, typically by computing the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define target variable y\n",
    "y = df['Discharge']\n",
    "y = np.log(y)\n",
    "\n",
    "# Select input features X (all except Discharge)\n",
    "X = df.drop(['Discharge', 'Name', 'Continent'], axis=1)\n",
    "\n",
    "# Apply log transformation to Area only\n",
    "X['Area'] = np.log(X['Area'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "\n",
    "# Compute the mean squared error of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Plot scatter of regression line, x_test vs y_test and y_pred\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True discharge (log)')\n",
    "plt.ylabel('Predicted discharge (log)')\n",
    "# Add mse score in title\n",
    "plt.title(f'MSE: {mse:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5** (optional)\n",
    "\n",
    "Can you improve the performance of the random forest regressor by fine-tuning its hyperparameters?\n",
    "* n_estimators: The number of trees in the forest.\n",
    "* max_depth: The maximum depth of each tree.\n",
    "* min_samples_split: The minimum number of samples required to split an internal node.\n",
    "* min_samples_leaf: The minimum number of samples required to be at a leaf node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
