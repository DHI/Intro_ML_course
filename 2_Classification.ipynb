{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFeafX0r5-Dz"
      },
      "source": [
        "# **Module 2 - Introduction to Classification with Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1FnWnWc5-D1"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/DHI/Intro_ML_course/blob/main/module_2/2_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this module on classification! In this section, we embark on a focused and practical exploration of Decision Trees applied to the specific task of predicting whether a river's average discharge exceeds or falls below 5,000m3/s. Our predictive framework will be based on two parameters: the river's length and drainage area.\n",
        "\n",
        "This exploration will extend beyond the basic understanding of Decision Trees, as we will delve into the intricacies of utilizing the bagging principle to develop Random Forest models. Additionally, we will deeply analyze model evaluation methodologies.\n",
        "\n",
        "This journey will not only provide a solid understanding of Decision Trees and their application in classification but also offer valuable insights into model assessment, enabling us to tackle challenges such as overfitting and underfitting.\n",
        "\n",
        "#### What is Classification?\n",
        "\n",
        "**Classification** is a supervised machine learning task where the goal is to assign a category or label to a given input based on the available data. It's used for a wide range of applications, from spam email detection to medical diagnosis and much more."
      ],
      "metadata": {
        "id": "BnsumqiC6R2O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nul2E2nW5-D1"
      },
      "source": [
        "#### Let's start by importing all the required functions from:\n",
        "- usual data science libraries\n",
        "- the excellent machine-learning library: scikit-learn\n",
        "\n",
        "Scikit-learn's website contains many examples, code snippets and tutorials: https://scikit-learn.org/stable/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VPkC-9x5-D2"
      },
      "outputs": [],
      "source": [
        "# Ordinary data science packages\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn functions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DECISION TREES"
      ],
      "metadata": {
        "id": "_YEidKSuk79j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0i3Zqm5-D2"
      },
      "source": [
        "## I - Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is extracted from this Wikipedia page: [the list of rivers by discharge ](https://en.wikipedia.org/wiki/List_of_rivers_by_discharge). For this notebook, we decided to start with using only two features - the river length and its drainage area - to predict whether or not a river's average discharge exceeds $5000 m^3/s$ or not. Using only two features allows to simply visualize models through decision boundaries.  "
      ],
      "metadata": {
        "id": "6k_VEbhHkpnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCmiRTpB5-D2"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('https://github.com/pase-dhi/ML-for-water-professionals/raw/main/Wikipedia_rivers.csv')\n",
        "\n",
        "# Rename features\n",
        "data['Length (km)'] = data['length_km']\n",
        "data['Drainage area(km2)'] = data['drainage_area']\n",
        "data['Discharge (m3/s)'] = data['average_discharge ']\n",
        "data[f'Discharge over 5000 m3/s'] = data['average_discharge '] > 5000\n",
        "\n",
        "# Define features and target variable\n",
        "X = data[['Length (km)', 'Drainage area(km2)']]\n",
        "y = data[f'Discharge over 5000 m3/s']\n",
        "X.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.head(5)"
      ],
      "metadata": {
        "id": "v27rDAh5_t-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "149QCK6k5-D3"
      },
      "source": [
        "Split the data into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbTkO9LD5-D3"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: The `train_test_split` function typically divides the data randomly into two sets, one for training the model and the other for testing its performance. To ensure the reproducibility of the same train-test split across different runs or by other individuals, we utilize the `random_state` variable. This variable acts as a seed to the random number generator, which in turn controls the randomness involved in the data splitting process. By setting a specific value for random_state, we essentially fix the randomness in the data splitting process, leading to the same train-test split every time the code is run with that particular seed."
      ],
      "metadata": {
        "id": "gQt7SvVF_4oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head(5)"
      ],
      "metadata": {
        "id": "HluRngD8Bkb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 1**\n",
        "\n",
        "*Plot the drainage area against the river length, with different colors depending on the value of the label (`Discharge over 5000 m3/s`)*\n"
      ],
      "metadata": {
        "id": "omoz0OYxDLiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## II - Training a decision tree"
      ],
      "metadata": {
        "id": "_EyZcIBCkt3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Decision Tree** is a versatile and interpretable model that can be used for both classification and regression tasks. It's called a \"tree\" because it resembles an inverted tree with branches (decisions) and leaves (outcomes).\n",
        "\n",
        "The decision tree is constructed through a series of binary decisions, leading to a final classification or value. This process makes it a highly interpretable model, which is beneficial for understanding the reasoning behind predictions.\n",
        "\n",
        "#### Anatomy of a Decision Tree\n",
        "\n",
        "- **Root Node**: The top node in the tree, representing the first decision.\n",
        "\n",
        "- **Internal Node**: The nodes between the root and leaves, each corresponding to a decision.\n",
        "\n",
        "- **Leaf Node**: The final nodes where a classification (or regression) outcome is assigned.\n",
        "\n",
        "- **Splitting Criteria**: The conditions used at each node to decide how to proceed down the tree.\n",
        "\n",
        "### Classification with Decision Trees\n",
        "\n",
        "In classification, Decision Trees use a set of features to determine the class of the input.\n",
        "Decision Trees partition the data at each internal node by selecting the feature and the threshold value that best separates the classes.\n",
        "\n",
        "Each leaf node is assigned to a class. During classification, an input traverses the tree by following the decisions until it reaches a leaf, which assigns the final class label.\n",
        "\n",
        "### Advantages of Decision Trees\n",
        "\n",
        "- **Interpretability**: Decision Trees are easy to visualize and understand. You can trace the path from root to leaf to see how decisions are made.\n",
        "\n",
        "- **Little pre-processing**: Scaling or centering features is not required to train a decision tree.\n",
        "\n",
        "- **Nonlinear Relationships**: They can capture nonlinear relationships between features and outcomes.\n",
        "\n",
        "- **Feature Importance**: Decision Trees provide a measure of feature importance, helping you identify the most influential features.\n",
        "\n",
        "- **Versatile**: Decision Trees can handle both classification and regression problems."
      ],
      "metadata": {
        "id": "KD-rIWZk8GuJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGWC7hMt5-D3"
      },
      "source": [
        "Let us set the maximum depth of the decision tree. This parameters which are fixed before the training of the model are called **hyperparameters**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GipjFaYq5-D3"
      },
      "outputs": [],
      "source": [
        "max_depth = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now see how easy it is to define and train a machine learning model:"
      ],
      "metadata": {
        "id": "XffTxR7pCEgS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkACXb7M5-D4"
      },
      "outputs": [],
      "source": [
        "tree_classifier = DecisionTreeClassifier(max_depth=max_depth, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENUIJQC05-D4"
      },
      "outputs": [],
      "source": [
        "tree_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Done! We now have a trained decision tree. Let's visualize it."
      ],
      "metadata": {
        "id": "sSosqMT7OzPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "_ = plot_tree(tree_classifier,\n",
        "              feature_names=['Length', 'Drainage area'],\n",
        "              class_names=[f'low dis.', f'high dis.'],\n",
        "              impurity=False,\n",
        "              filled=True,\n",
        "              fontsize=8, ax=ax)"
      ],
      "metadata": {
        "id": "v8TtE-PZPW_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 2**\n",
        "\n",
        "*Try other hyperparameters for the decision tree (`max_depth`, `min_samples_split` and `min_samples_leaf`), how does it affect the shape of the tree? What happens if you do not use any hyperparameter: `tree_classifier = DecisionTreeClassifier()`?*\n",
        "\n",
        "--> [scikit-learn documentation on decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
      ],
      "metadata": {
        "id": "WMHS3ZcEUZ1Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_2nc2hI5-D4"
      },
      "source": [
        "Now, we can readily generate predictions for the test set. Let's test this by making predictions on the initial 5 elements of the test dataset and\n",
        "calculating the accuracy score for these 5 predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head(5)"
      ],
      "metadata": {
        "id": "cgYyYLbWS4BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQGfZnke5-D4"
      },
      "outputs": [],
      "source": [
        "tree_predictions = tree_classifier.predict(X_test[0:5])\n",
        "tree_predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0:5]"
      ],
      "metadata": {
        "id": "Hd_R7BC3S0PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(tree_predictions == y_test[0:5])"
      ],
      "metadata": {
        "id": "FUPIEoqkVV-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(tree_predictions, y_test[0:5])"
      ],
      "metadata": {
        "id": "6rUNX9uaTqsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKyT4eQC5-D4"
      },
      "source": [
        "## III. Visualize the decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOhSYEnl5-D4"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(clf, X, y, axes, score=False, legend=False, display_names=False):\n",
        "\n",
        "    # Create a mesh grid to plot decision boundaries\n",
        "    x_min, x_max, y_min, y_max = axes\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
        "\n",
        "    # Predict on the mesh grid\n",
        "    if score:\n",
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "    else:\n",
        "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlGn')\n",
        "    plt.plot(X.iloc[:, 0][y==0], X.iloc[:, 1][y==0], \"ro\", label=\"Lower discharge\")\n",
        "    plt.plot(X.iloc[:, 0][y==1], X.iloc[:, 1][y==1], \"g^\", label=\"Higher discharge\")\n",
        "    plt.xlabel(\"Length ($km$)\", fontsize=14)\n",
        "    plt.ylabel(\"Drainage area ($km^2$)\", fontsize=14)\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following figure displays the training dataset as well as the decision boundaries of the trained decision tree"
      ],
      "metadata": {
        "id": "gYBuHhKsWGzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(6, 6))\n",
        "plot_decision_boundary(tree_classifier, X_train, y_train,\n",
        "                       axes=[np.min(X.iloc[:, 0]), np.max(X.iloc[:, 0]), np.min(X.iloc[:, 1]), np.max(X.iloc[:, 1])])\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title('Training dataset')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ZuNYF6LnV_Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kYq1r6g5-D4"
      },
      "source": [
        "Let's look at the score on the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD_lHl8-5-D4"
      },
      "outputs": [],
      "source": [
        "train_predictions = tree_classifier.predict(X_train)\n",
        "train_accuracy = accuracy_score(train_predictions, y_train)\n",
        "print('Accuracy on the training dataset', train_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should plot the test dataset to see how does the model generalizes to instances outside of the training dataset:"
      ],
      "metadata": {
        "id": "lzN7SOdbWWCy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLoU86Xo5-D5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plot_decision_boundary(tree_classifier, X_test, y_test,\n",
        "                       axes=[np.min(X.iloc[:, 0]), np.max(X.iloc[:, 0]), np.min(X.iloc[:, 1]), np.max(X.iloc[:, 1])])\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title('Test dataset')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3IJtE-T5-D5"
      },
      "outputs": [],
      "source": [
        "test_predictions = tree_classifier.predict(X_test)\n",
        "test_accuracy = accuracy_score(test_predictions, y_test)\n",
        "print('Accuracy on the test dataset', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 3**\n",
        "\n",
        "*Is there a big difference between the performance in the training dataset and the test dataset? Why? Try with other hyperparameters (`max_depth=10`, `max_depth=1`, etc.), how does this affect the performance on both datasets?*"
      ],
      "metadata": {
        "id": "POP5wukNWc_a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LAReeDO5-D5"
      },
      "source": [
        "## IV - Dependence of the performance on hyperparameters and on the number of training points."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does accuracy on both the training and the test dataset depend on the `max_depth` of the decision tree?"
      ],
      "metadata": {
        "id": "YpaMOJ-bXx1W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G33WhJxM5-D5"
      },
      "outputs": [],
      "source": [
        "# Define a range of max_depth values to loop over\n",
        "max_depth_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "# Initialize lists to store accuracy scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "# Loop over max_depth values\n",
        "for max_depth in max_depth_values:\n",
        "    # Create and fit the decision tree classifier\n",
        "    clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions on training and testing data\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy scores for training and testing data\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    # Append accuracy scores to the lists\n",
        "    train_scores.append(train_accuracy)\n",
        "    test_scores.append(test_accuracy)\n",
        "\n",
        "# Plot the bias-variance trade-off\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depth_values, train_scores, label='Train Accuracy', marker='o')\n",
        "plt.plot(max_depth_values, test_scores, label='Test Accuracy', marker='o')\n",
        "plt.xlabel('Max Depth of Decision Tree')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Bias-Variance Trade-off')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 4**\n",
        "\n",
        "*You can replicate the following figure but this time, instead of changing `max_depth`, loop over multiple values of `min_samples_split`. For what value of both parameters does the model start to overfit?*"
      ],
      "metadata": {
        "id": "lKkye1-qYI8m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T41CteLo5-D6"
      },
      "source": [
        "## V - Dependence of scores on the size of the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to the hyperparameters of the model, the amount of data used for training have an important impact on the performance, let us train decision trees with identical hyperparamaters but with an increasing number of training points."
      ],
      "metadata": {
        "id": "F8iZUHcaY3zi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDuu6Uqx5-D6"
      },
      "outputs": [],
      "source": [
        "max_depth=2\n",
        "\n",
        "# Define a range of training dataset sizes to loop over\n",
        "train_sizes = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "# Initialize lists to store accuracy scores\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "# Loop over training dataset sizes\n",
        "for train_size in train_sizes:\n",
        "    # Split the dataset into training and testing sets with the specified size\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1 - train_size),\n",
        "                                                        random_state=42)\n",
        "\n",
        "    # Create and fit the decision tree classifier with the constant max_depth\n",
        "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions on training and testing data\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy scores for training and testing data\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    # Append accuracy scores to the lists\n",
        "    train_scores.append(train_accuracy)\n",
        "    test_scores.append(test_accuracy)\n",
        "\n",
        "# Plot the bias-variance trade-off based on training dataset size\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores, label='Train Accuracy', marker='o')\n",
        "plt.plot(train_sizes, test_scores, label='Test Accuracy', marker='o')\n",
        "plt.xlabel('Training Dataset Size (Proportion)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Performance vs Training Datset Size')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 5**\n",
        "\n",
        "*Does this behave as you would expect, does the performance saturate and how do hyperparameters affect these curves?*"
      ],
      "metadata": {
        "id": "dvHfbr_wZZ9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FORESTS"
      ],
      "metadata": {
        "id": "xfmaYVoclAeZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOnNbh0H5-D6"
      },
      "source": [
        "In the previous section, we explored Decision Trees as a simple yet interpretable model for classification. Decision Trees have their strengths, but they are prone to overfitting, which can lead to poor generalization. Random Forests, a powerful ensemble learning method, can help mitigate this issue while improving predictive accuracy.\n",
        "\n",
        "## What Are Random Forests?\n",
        "\n",
        "Random Forests are an ensemble method that combines multiple Decision Trees to create a more robust and accurate model. They are particularly effective for classification tasks.\n",
        "\n",
        "### The Idea Behind Random Forests\n",
        "\n",
        "The core idea behind Random Forests is **bagging** (Bootstrap Aggregating) and **feature randomization**:\n",
        "\n",
        "1. **Bagging**: Random Forests build multiple Decision Trees, each trained on a random subset of the training data. This random sampling reduces the risk of overfitting.\n",
        "\n",
        "2. **Feature Randomization**: For each split in the Decision Trees, only a random subset of features is considered. This introduces diversity among the trees, improving the model's ability to generalize.\n",
        "\n",
        "**Note**: *For this example, feature randomization is not relevant as we only use two features.*\n",
        "\n",
        "### Combining Multiple Decision Trees\n",
        "\n",
        "Random Forests aggregate the predictions of multiple Decision Trees to make a final classification decision:\n",
        "\n",
        "- For classification tasks, it employs a **majority vote** mechanism. The class most frequently predicted by the individual trees becomes the predicted class.\n",
        "\n",
        "- For regression tasks, it averages the predictions made by individual trees.\n",
        "\n",
        "## Advantages of Random Forests\n",
        "\n",
        "Random Forests offer several advantages:\n",
        "\n",
        "1. **Improved Generalization**: By combining the results of multiple Decision Trees, overfitting is reduced.\n",
        "\n",
        "2. **High Accuracy**: Random Forests can provide high accuracy, making them suitable for complex classification tasks.\n",
        "\n",
        "3. **Feature Importance**: They can help identify the most important features in the dataset.\n",
        "\n",
        "4. **Robustness**: Random Forests are robust to outliers and noise in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipfu-Nfi5-D6"
      },
      "source": [
        "## I. Hard-coding of bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bJXhsaa5-D6"
      },
      "source": [
        "We define 100 decision trees and train each one of them on 20 randomly chosen instances in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_trees = 100\n",
        "n_instances = 20\n",
        "forest = [DecisionTreeClassifier() for _ in range(n_trees)]"
      ],
      "metadata": {
        "id": "X1oj44S4meGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the choice function from numpy to randomly select training instances.\n",
        "\n",
        "**Note**: `replace=True` means that each training instance can be chosen multiple times. A random sample *with replacement* is called a bootstrap sample and Bagging stands for \"Boostrap aggregating\""
      ],
      "metadata": {
        "id": "TQzioWuAnnF3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRahkkVA5-D6"
      },
      "outputs": [],
      "source": [
        "idx = np.random.choice(X_train.index, size=n_instances, replace=True)\n",
        "idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the in-bag dataset\n",
        "X_in_bag, y_in_bag = X_train.loc[idx], y_train.loc[idx]\n",
        "X_in_bag.head(3)"
      ],
      "metadata": {
        "id": "SC_gRuDhsg81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5AK-b-E5-D6"
      },
      "source": [
        "Let us now train each `tree` in `forest` on a different set of instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9Aatt9Q5-D7"
      },
      "outputs": [],
      "source": [
        "test_scores = []\n",
        "oob_scores = []\n",
        "\n",
        "for tree in forest:\n",
        "  idx = np.random.choice(X_train.index, size=50, replace=True)\n",
        "  X_in_bag, y_in_bag = X_train.loc[idx], y_train.loc[idx]\n",
        "  tree.fit(X_in_bag, y_in_bag)\n",
        "\n",
        "  # let us compute the performance on the test dataset\n",
        "  test_score = accuracy_score(tree.predict(X_test), y_test)\n",
        "  test_scores.append(test_score)\n",
        "\n",
        "  # remark - many instances on the training dataset are not used for training\n",
        "  # we can compute the accuracy scores on those\n",
        "  X_out_of_bag, y_out_of_bag = X_train.drop(idx), y_train.drop(idx)\n",
        "  out_of_bag_score = accuracy_score(tree.predict(X_out_of_bag), y_out_of_bag)\n",
        "  oob_scores.append(out_of_bag_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "plt.hist(test_scores, bins=5)\n",
        "plt.xlabel('Accuracy on the test dataset')\n",
        "plt.ylabel('Frequency')"
      ],
      "metadata": {
        "id": "0AFVAaRWq372"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Average score of individual decision trees', np.mean(test_scores))"
      ],
      "metadata": {
        "id": "UyLbzh-9rSP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 6**\n",
        "\n",
        "*Display the out of bag scores*\n",
        "\n",
        "You should obtain a similar result. The out-of-bag instances allow to check how the model generalizes to new instances, and allows to have any test data which is very useful for small datasets (see more about out-of-bag scores below)."
      ],
      "metadata": {
        "id": "_Y-7mXm7trZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Average score of individual decision trees', np.mean(oob_scores))"
      ],
      "metadata": {
        "id": "967eZGLWo46y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UjBDVZU5-D7"
      },
      "source": [
        "**Now comes the magic of random forests**. For each test set instance, generate the predictions of the 100 Decision Trees, and keep only the most frequent prediction. This gives you _majority-vote predictions_ over the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvQZ-1JB5-D7"
      },
      "outputs": [],
      "source": [
        "Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
        "\n",
        "for tree_index, tree in enumerate(forest):\n",
        "    Y_pred[tree_index] = tree.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCp-luDk5-EB"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X00U3Fq5-EC"
      },
      "source": [
        "Evaluate these predictions on the test set: you should obtain a higher accuracy than individual decision trees.\n",
        "\n",
        "Congratulations, you have trained a Random Forest classifier!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm-yqP4c5-EC"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise 7**\n",
        "\n",
        "*How does this compare to the best decision you have trained in the previous section? Try to include some hyperparameters to the decision trees instead of the uncronstrained* `forest = [DecisionTreeClassifier() for _ in range(n_trees)]`"
      ],
      "metadata": {
        "id": "bYdZ5LaPv560"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jft_HB1B5-EC"
      },
      "source": [
        "## II. Using sklearn's Bagging and RandomForest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of create the previous loop over the trees, you can use `scikit-learn`'s `RandomForestClassifier` or `BaggingClassifier`. Here's how:"
      ],
      "metadata": {
        "id": "6dHFhNtZwqm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9imrBIc5-EC"
      },
      "outputs": [],
      "source": [
        "# Option 1\n",
        "rf_clf = RandomForestClassifier(n_estimators=100,\n",
        "                                max_samples=20,\n",
        "                                max_features=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# Option 2\n",
        "rf_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                              max_samples=20,\n",
        "                              n_estimators=100)\n",
        "```"
      ],
      "metadata": {
        "id": "oSDb3ZzmycLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcBjE30j5-EC"
      },
      "outputs": [],
      "source": [
        "rf_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the decision boundary of this random forest"
      ],
      "metadata": {
        "id": "ag_OamV0w8mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plot_decision_boundary(rf_clf, X_train, y_train,\n",
        "                       axes=[np.min(X.iloc[:, 0]), np.max(X.iloc[:, 0]), np.min(X.iloc[:, 1]), np.max(X.iloc[:, 1])])\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "fLiIKIowyodY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests result from a vote, instead of the majority vote, we can display the number of trees which voted for \"high discharge\", these are referred to as **class probabilities**."
      ],
      "metadata": {
        "id": "pnxMdAT8yqkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicitons\n",
        "rf_clf.predict(X_test[:5])"
      ],
      "metadata": {
        "id": "tEEBxC4yzOnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicitons probabilities\n",
        "rf_clf.predict_proba(X_test[:5])[:, 1]"
      ],
      "metadata": {
        "id": "vMoKMoz6zajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how those probabilities look against the training set:"
      ],
      "metadata": {
        "id": "Hm6SK4pQz1l5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBhwy_1I5-ED"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plot_decision_boundary(rf_clf, X_train, y_train, score=True,\n",
        "                       axes=[np.min(X.iloc[:, 0]), np.max(X.iloc[:, 0]), np.min(X.iloc[:, 1]), np.max(X.iloc[:, 1])])\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III - Out-of-bag score"
      ],
      "metadata": {
        "id": "B2EXqRlQ64a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A standout feature of Random Forest is its ability to estimate model performance without requiring a separate test dataset, thanks to the Out-of-Bag (OOB) score.\n",
        "\n",
        "#### **Understanding the OOB Score**\n",
        "\n",
        "The OOB score evaluates the Random Forest model during training in the following way:\n",
        "\n",
        "- Each decision tree in the Random Forest is trained on a subsample of the original dataset, allowing some data points to be excluded and others to be duplicated in the training set of each tree.\n",
        "\n",
        "- The OOB score is computed by assessing each data point using the decision trees that didn't include it during training, or were \"out of bag.\"\n",
        "\n",
        "- The OOB score aggregates the predictions from these out-of-bag trees, it assigns the most common class among the out-of-bag predictions to the data point.\n",
        "\n",
        "It serves as a valuable metric for evaluating model performance, particularly when working with limited data or aiming to assess the model's capabilities without setting aside a separate validation dataset."
      ],
      "metadata": {
        "id": "tqJnuezD7WIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oob_scores = []\n",
        "forest = RandomForestClassifier(max_samples=20, oob_score=True, random_state=0)\n",
        "\n",
        "for n_estimators in range(1, 50, 4):\n",
        "    forest.set_params(n_estimators=n_estimators)\n",
        "    forest.fit(X, y)  # Fit the model to the full dataset this time\n",
        "    oob_scores.append(forest.oob_score_)"
      ],
      "metadata": {
        "id": "_FvSBHGf8lWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the OOB score vs the number of trees\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(range(1, 50, 4), oob_scores, marker='o')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('OOB Score')\n",
        "plt.title('OOB Score vs. Number of Trees in Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2QERLw08_QkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OOB score is supposed to generally increase as the number of trees increases, but you will probably see some variability in the values."
      ],
      "metadata": {
        "id": "Pswl8XLa9Rtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV - Finding the best hyperpareters using GridSearchCV"
      ],
      "metadata": {
        "id": "G2LxHRK94fwQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58U1RZp85-EG"
      },
      "source": [
        "With parameters such as `max_depth`, `max_samples`, `n_estimators`, and others, the Random Forest algorithm can have a wide range of potential hyperparameter combinations. The `GridSearchCV` method efficiently explores this hyperparameter space to find the best combination. The `CV` in `GridSearchCV` stands for cross-validation:\n",
        "\n",
        "Cross-validation involves partitioning the dataset into multiple subsets, performing model training and evaluation iteratively, ensuring that every data point is part of the validation set at least once. This process helps in assessing the model's performance more accurately and robustly.\n",
        "\n",
        "`GridSearchCV` simplifies the complex task of selecting the best set of hyperparameters by automatically performing cross-validation for each combination. It computes the model's performance metrics for different splits of the data, allowing practitioners to identify the hyperparameter configuration that yields the best performance across the dataset.\n",
        "\n",
        "For more information about cross-validation, the following scikit-learn tutorial is a great resource: https://scikit-learn.org/stable/modules/cross_validation.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MmcbfGe5-EG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a range of hyperparameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 20],\n",
        "    'max_depth': [None, 3, 4, 5],\n",
        "    'max_samples': [None, 10, 20, 30, 50]\n",
        "}\n",
        "\n",
        "# Create a Random Forests classifier\n",
        "tree_classifier = RandomForestClassifier()\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=tree_classifier, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=3)\n",
        "\n",
        "# Perform the grid search on the training data\n",
        "grid_search.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the results of this GridSearch as a heatmap."
      ],
      "metadata": {
        "id": "AHI_V6ZhCtyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the results of the grid search into a DataFrame\n",
        "results = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Extract the hyperparameters and the mean test scores\n",
        "params = results[['param_n_estimators', 'param_max_depth', 'param_max_samples']]\n",
        "params = params.astype(str)\n",
        "params['mean_test_score'] = results['mean_test_score']\n",
        "\n",
        "# Create a pivot table for the heatmap\n",
        "heatmap_data = params.pivot_table(index='param_max_depth', columns='param_max_samples', values='mean_test_score')\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Mean Test Score'})\n",
        "plt.title('Grid Search Results')\n",
        "plt.xlabel('Number of samples')\n",
        "plt.ylabel('Maximum depth')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bm-RY9ecCNm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select the best result only."
      ],
      "metadata": {
        "id": "BgrmcnFxC2g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "2E5iVINJCzWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "id": "o4xtdQaoEDcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nReeVBn95-EG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plot_decision_boundary(best_model, X, y, score=True,\n",
        "                       axes=[np.min(X.iloc[:, 0]), np.max(X.iloc[:, 0]), np.min(X.iloc[:, 1]), np.max(X.iloc[:, 1])])\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now, we have tried to maximize the test (or cross-validation) accuracy, yet the accuracy score is often not the best metric to optimize. Recall, precision, f1-score and ROC-AUC are important metrics to consider..."
      ],
      "metadata": {
        "id": "4hJRfdKa4T0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFUSION MATRICES"
      ],
      "metadata": {
        "id": "Bzr-ioRIFJA4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpQ3M1Xi5-EG"
      },
      "source": [
        "In classification tasks, it's essential to understand how well a model is performing. And the `accuracy_score` which we used until now is often not the best metric to determine the performance.\n",
        "\n",
        "Consider the example of a rare medical condition that only affects a small percentage of the population. If a classifier simply predicts that all patients are healthy, it can achieve a high accuracy score, as it correctly classifies the majority class (healthy individuals). However, this approach completely fails to identify the presence of the rare disease for the few patients which do carry it.\n",
        "\n",
        "#### **Confusion Matrix**\n",
        "\n",
        "A **Confusion Matrix** is a table that allows us to understand the performance of a classification model. It summarizes the model's predictions against actual outcomes. The confusion matrix consists of four components:\n",
        "\n",
        "- **True Positives (TP)**: The number of correct positive predictions.\n",
        "- **True Negatives (TN)**: The number of correct negative predictions.\n",
        "- **False Positives (FP)**: The number of incorrect positive predictions (Type I error).\n",
        "- **False Negatives (FN)**: The number of incorrect negative predictions (Type II error)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to sklearn library, we can split the data, define and train a random forest and predict classes and probabilities, all in 4 lines of code:"
      ],
      "metadata": {
        "id": "GpaE0Z0-GBUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I- Display the confusion matrix"
      ],
      "metadata": {
        "id": "0uRqHLr4Gtnq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsNskkbN5-EH"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(max_depth=2,\n",
        "                             max_samples=30,\n",
        "                             n_estimators=10,\n",
        "                             random_state=42).fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xticks([0.5, 1.5], [f'Lower discharge', f'Higher discharge'])\n",
        "plt.yticks([0.3, 1.3], [f'Lower discharge', f'Higher discharge'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6FattOmpGrHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Deduce recall and precision"
      ],
      "metadata": {
        "id": "90e_hZ_XJNcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using the confusion matrix, we can calculate various metrics to assess model performance:\n",
        "\n",
        "#### **Recall**\n",
        "\n",
        "**Recall** also known as Sensitivity or True Positive Rate, is a metric that answers the question: \"Of all the actual high discharge rivers, how many did the model identify?\" It is calculated as:\n",
        "\n",
        "`Recall = TP / (TP + FN)`\n",
        "\n",
        "High recall indicates that the model is good at capturing positive cases, minimizing false negatives.\n",
        "\n",
        "#### **Precision**\n",
        "\n",
        "**Precision** answers the question: \"Of all the predicted high discharge rivers, how many actually had high discharge?\" It measures the accuracy of positive predictions and is calculated as:\n",
        "\n",
        "`Precision = TP / (TP + FP)`\n",
        "\n",
        "High precision indicates that the model's positive predictions are reliable and have a low rate of false positives.\n",
        "\n",
        "#### **F1-Score**\n",
        "\n",
        "**F1-score** is a harmonic mean of precision and recall and provides a balanced assessment of the model's performance, especially when dealing with imbalanced datasets. It considers both false positives and false negatives, making it a useful metric for evaluating models in situations where we want to car equally about recall and precision. F1-score is calculated as:\n",
        "\n",
        "`F1-score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
        "\n",
        "A high F1-score indicates that the model has both good precision and recall, implying that it has a balance between minimizing false positives and false negatives."
      ],
      "metadata": {
        "id": "RMi1mAzcKIq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate recall\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "# Calculate f1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"f1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "ntiecfdkI3qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 9**\n",
        "\n",
        "*Deduce the recall and precision from the decision matrix. It should match the previous numbers.*"
      ],
      "metadata": {
        "id": "DkHgArTfItMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Change the discrimination threshold"
      ],
      "metadata": {
        "id": "FFdNAPa0JRwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, we predict a river as \"high discharge\" if the probability is higher than 0.5. But we might want to define another threshold.\n",
        "\n",
        "In the example of the medical diagnosis, we might want to conduct additional tests if the probability of someone having a malignant disease is higher than, say, 10% as we prefer to have false positive than false negatives."
      ],
      "metadata": {
        "id": "nxXW00XtHaIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred == (y_probs > 0.5)"
      ],
      "metadata": {
        "id": "xwGh3e77HPxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpiPCoU35-EH"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "conf_matrix = confusion_matrix(y_test, y_probs>0.7)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xticks([0.5, 1.5], [f'Lower discharge', f'Higher discharge'])\n",
        "plt.yticks([0.3, 1.3], [f'Lower discharge', f'Higher discharge'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 10**\n",
        "\n",
        "*By setting the discrimination threshold to 0.7 (or 0.3), how are recall, precision and f1-score affected?*"
      ],
      "metadata": {
        "id": "-HIwtFUGIS_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV - ROC curve"
      ],
      "metadata": {
        "id": "4X5o6C_KJww3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Receiver Operating Characteristic (ROC) Curve**\n",
        "\n",
        "The **ROC Curve** is a graphical representation of a model's performance as the discrimination threshold is varied. It plots the True Positive Rate (Recall) against the False Positive Rate (FPR). Each point on the ROC curve represents a trade-off between recall and precision.\n",
        "\n",
        "**False Positive Rate (FPR)**: FPR measures the rate of incorrectly predicted positive cases among all the actual negative cases. It is calculated as:\n",
        "\n",
        "$FPR = \\frac{FP}{FP + TN}$\n",
        "\n",
        "A low FPR indicates that the model makes fewer false positive errors.\n",
        "\n",
        "#### **Area Under the ROC Curve (AUC-ROC)**\n",
        "\n",
        "The **Area Under the ROC Curve (AUC-ROC)** is a single value that summarizes the ROC curve's performance. A higher AUC-ROC indicates a better-performing model. If the AUC-ROC is 0.5, it suggests that the model's performance is equivalent to random guessing, while an AUC-ROC of 1.0 means perfect discrimination. AUC-ROC is generally a good metric to compare different models as it provides a single-value assessment of a model's ability to discriminate between positive and negative classes across all possible classification thresholds."
      ],
      "metadata": {
        "id": "sXbtEz14KEk1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWIYiUje5-EH"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve\n",
        "fpr, recall, thresholds = roc_curve(y_test, y_probs)\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "roc_auc = auc(fpr, recall)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, recall, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excercise 11**\n",
        "\n",
        "*What should be the ROC curve of a random model which predictions are completely arbitrary?*"
      ],
      "metadata": {
        "id": "fTYAslx8LN0X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTPs8bog5-EH"
      },
      "outputs": [],
      "source": [
        "# Random predictions\n",
        "y_probs_random = np.random.rand(len(y_test))\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, recall, thresholds = roc_curve(y_test, y_probs_random)\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "roc_auc = auc(fpr, recall)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, recall, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you try to run the previous code (with the random predictions) multiple times, you will see that the performance varies significantly. This high variability is linked to the small size of the dataset."
      ],
      "metadata": {
        "id": "Uj29Teum5bxD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzX6CEzl5-EI"
      },
      "source": [
        "# FEATURE IMPORTANCE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest models provide a feature importance metric, which quantifies the relevance of each feature in the model's predictive performance. By analyzing the feature importance, we can identify the most influential features driving the model's decisions, gaining valuable insights into the underlying data relationships.\n",
        "\n",
        "The feature importance score is computed based on the decrease in node impurity achieved by each feature when building the Random Forest. Features with higher importance values have a more substantial impact on the model's predictions, aiding in feature selection and enhancing the interpretability of the model's decision-making process."
      ],
      "metadata": {
        "id": "PZVX_bv6M7Q3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-ekla-E5-EI"
      },
      "outputs": [],
      "source": [
        "# Calculate and visualize feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "features = X.columns\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.subplots_adjust(left=0.3)\n",
        "plt.barh(features, feature_importances)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importance in Random Forest Classifier')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Here, we are limited by the very low number of features (2), but feature importance can be very useful to determine which set of features are useful or useless to the model.\n",
        "\n",
        "When dealing with high-dimensional data, understanding feature importance can guide feature selection and help in building more efficient and interpretable models. Furthermore, feature importance analysis aids in identifying potential data redundancies, improving model generalization, and reducing overfitting risks.\n",
        "\n",
        "Moreover, feature importance analysis can contribute to domain-specific insights, enabling a better understanding of the underlying data dynamics."
      ],
      "metadata": {
        "id": "JZU1x-TfNAKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **THE END**"
      ],
      "metadata": {
        "id": "KOdqDYps6EZT"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 (DHI GRAS)",
      "language": "python",
      "name": "py3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
